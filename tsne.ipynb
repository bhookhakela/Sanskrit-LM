{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67b0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter  \n",
    "import os\n",
    "import io\n",
    "import PIL.Image\n",
    "from matplotlib import font_manager\n",
    "\n",
    "def create_tsne_visualization_tensorboard(model, word_to_index, index_to_word, words_to_visualize=None,\n",
    "                                         perplexity=30, n_iter=1000, random_state=42,\n",
    "                                         log_dir=\"runs/sanskrit_tsne\", font_path=None):\n",
    "    embedding_matrix = model.hidden.weight.data.T  \n",
    "    if words_to_visualize is None:\n",
    "        words_to_visualize = list(word_to_index.keys())\n",
    "    else:\n",
    "        words_to_visualize = [word for word in words_to_visualize if word in word_to_index]\n",
    "    indices_to_visualize = [word_to_index[word] for word in words_to_visualize]\n",
    "    embeddings_to_visualize = embedding_matrix[indices_to_visualize]  \n",
    "    tsne = TSNE(n_components=3, random_state=random_state, perplexity=perplexity, n_iter=n_iter)\n",
    "    tsne_results = tsne.fit_transform(embeddings_to_visualize.cpu().numpy())  \n",
    "    writer = SummaryWriter(log_dir)\n",
    "    writer.add_embedding(embeddings_to_visualize,\n",
    "                        metadata=words_to_visualize,\n",
    "                        tag=\"sanskrit_word_embeddings\")\n",
    "    print(f\"Saving TensorBoard data to {log_dir}.  Open TensorBoard to visualize the embeddings.\")\n",
    "    writer.close()\n",
    "\n",
    "def create_sprite_image(dict_map, image_width=32, image_height=32):\n",
    "    num_images = len(dict_map)\n",
    "    n_cols = int(np.ceil(np.sqrt(num_images)))\n",
    "    n_rows = int(np.ceil(num_images / n_cols))\n",
    "    sprite_image = np.full((n_rows * image_height, n_cols * image_width, 4), [255, 255, 255, 0], dtype=np.uint8) \n",
    "    labels = []\n",
    "    font_path = \"Arial Unicode.ttf\"  \n",
    "    try:\n",
    "        font_manager.findfont(\"Arial Unicode\")\n",
    "    except:\n",
    "        font_path = \"FreeSerif.ttf\" \n",
    "    try:\n",
    "        font_manager.findfont(\"FreeSerif\")\n",
    "    except:\n",
    "        print(\"No appropriate font found.  Please install a Unicode font (e.g., Arial Unicode MS, FreeSerif) for proper Sanskrit display.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, 20)  \n",
    "    except OSError as e:\n",
    "        print(f\"Error loading font: {e}\")\n",
    "        print(\"Please ensure the font file is available at the specified path.\")\n",
    "        return None, None\n",
    "\n",
    "    for i, (index, text) in enumerate(dict_map.items()):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        x1 = col * image_width\n",
    "        y1 = row * image_height\n",
    "        x2 = x1 + image_width\n",
    "        y2 = y1 + image_height\n",
    "        img = PIL.Image.new('RGBA', (image_width, image_height), color=(255, 255, 255, 0))  \n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.text((0, 0), text, font=font, fill=(0, 0, 0, 255))  \n",
    "        sprite_image[y1:y2, x1:x2] = np.array(img)\n",
    "        labels.append(text)\n",
    "\n",
    "    return sprite_image, labels\n",
    "\n",
    "def visualize_embeddings(model, word_to_index, index_to_word, log_dir=\"runs/embedding_visualization\",\n",
    "                         max_words=1000, image_width=32, image_height=32):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    embedding_matrix = model.hidden.weight.data.T\n",
    "\n",
    "    if len(word_to_index) > max_words:\n",
    "        print(f\"Visualizing a subset of {max_words} words from the vocabulary.\")\n",
    "        word_to_index_subset = dict(list(word_to_index.items())[:max_words])\n",
    "        index_to_word_subset = {v: k for k, v in word_to_index_subset.items()}\n",
    "        indices_to_visualize = list(word_to_index_subset.values())\n",
    "        embeddings_to_visualize = embedding_matrix[indices_to_visualize]\n",
    "        metadata = list(word_to_index_subset.keys())\n",
    "\n",
    "    else:\n",
    "        embeddings_to_visualize = embedding_matrix\n",
    "        metadata = list(word_to_index.keys())\n",
    "\n",
    "    sprite, labels = create_sprite_image(index_to_word, image_width, image_height)\n",
    "\n",
    "    if sprite is not None:  \n",
    "        writer.add_image('sanskrit_word_embeddings_sprite', sprite, dataformats='NHWC')\n",
    "        writer.add_embedding(embeddings_to_visualize,\n",
    "                            metadata=labels,  \n",
    "                            label_img=sprite.reshape(-1, image_height, image_width, 4),\n",
    "                            tag='sanskrit_word_embeddings')\n",
    "        print(f\"Saving TensorBoard data to {log_dir}.  Open TensorBoard to visualize the embeddings.\")\n",
    "    else:\n",
    "        writer.add_embedding(embeddings_to_visualize,\n",
    "                            metadata=labels,  \n",
    "                            tag='sanskrit_word_embeddings')\n",
    "        print(f\"Saving TensorBoard data to {log_dir}.  Open TensorBoard to visualize the embeddings (without sprite).\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da9d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "text_dir = 'Sans_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91bc2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for file_name in os.listdir(text_dir):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        with open(os.path.join(text_dir, file_name), 'r', encoding='utf-8') as f:\n",
    "            all_text += f.read() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e89eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = re.sub(r'\\s+', ' ', all_text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29a8548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = all_text.split()\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef86bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35761ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 7  \n",
    "vocab = [word for word, freq in word_counts.items() if freq >= min_freq]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa5bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdc9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: i for i, word in enumerate(vocab)}\n",
    "index2word = {i: word for word, i in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "956ed9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [w for w in tokens if w in word2index]\n",
    "data = []\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccf80499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecScratch(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2VecScratch, self).__init__()\n",
    "    self.hidden = nn.Linear(vocab_size, embedding_dim, bias = False)\n",
    "    self.output = nn.Linear(embedding_dim, vocab_size, bias = False)\n",
    "  def forward(self, x):\n",
    "    x = self.hidden(x)\n",
    "    x = self.output(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5979dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Word2VecScratch(vocab_size, 500).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9878b325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343297/2108987895.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('best_model.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('best_model.pth')\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7edfb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['Arial Unicode'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['FreeSerif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading font: cannot open resource\n",
      "Please ensure the font file is available at the specified path.\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "Saving TensorBoard data to runs/sanskrit_embeddings_demo.  Open TensorBoard to visualize the embeddings (without sprite).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Venv/SanskritLLM/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "Saving TensorBoard data to runs/sanskrit_tsne_demo.  Open TensorBoard to visualize the embeddings.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "visualize_embeddings(model, word2index, index2word, log_dir=\"runs/sanskrit_embeddings_demo\", max_words=vocab_size)\n",
    "\n",
    "create_tsne_visualization_tensorboard(model, word2index, index2word, log_dir=\"runs/sanskrit_tsne_demo\", words_to_visualize=list(word2index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c51e598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'सर्वात्मने'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[7664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed1d5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'गोविन्दाय'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[7649]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308f410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sanskritllm",
   "language": "python",
   "name": "sanskritllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
