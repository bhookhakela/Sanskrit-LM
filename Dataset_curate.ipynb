{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# List of VARIABLE strings\n",
    "variables = [\n",
    "    \"agastya\",\n",
    "    \"ananda-tirtha\",\n",
    "    \"annamacharya\",\n",
    "    \"appayya-dikshita\",\n",
    "    # \"brahmananda\",  # commented out\n",
    "    \"chanakya\",\n",
    "    \"chandrashekharabharati\",\n",
    "    \"ganapati-muni\",\n",
    "    \"gaudapaada\",\n",
    "    \"jagadisha-shastri\",\n",
    "    \"kalidasa\",\n",
    "    \"krishnanandasarasvati\",\n",
    "    \"bhartrihari\",\n",
    "    \"madhusudanasarasvati\",\n",
    "    \"maheshvaranandasarasvati\",\n",
    "    \"muttusvami-dikshitara\",\n",
    "    \"nrisimhabharatisvami\",\n",
    "    \"pandita-bellamkonda-ramaraya-kavindra\",\n",
    "    \"pushpadanta\",\n",
    "    \"ramana-maharshi\",\n",
    "    \"ramanuja\",\n",
    "    \"sachchidananda-shivabhinava-nrisimhabharati\",\n",
    "    \"sadashivabrahmendra\",\n",
    "    \"samartha-ramadasa\",\n",
    "    \"shankaracharya\",\n",
    "    \"shridhara-venkatesha\",\n",
    "    \"tyagaraja\",\n",
    "    \"vadiraja\",\n",
    "    \"vallabhaachaarya\",\n",
    "    \"valmiki\",\n",
    "    \"vangipuram-narasinhacharya\",\n",
    "    \"varahamihira\",\n",
    "    \"vasudevananda-sarasvati\",\n",
    "    \"vedanta-deshika\",\n",
    "    \"vivekananda\",\n",
    "    \"vyasa\",\n",
    "    \"yogananda\"\n",
    "]\n",
    "\n",
    "# Base URL pattern\n",
    "base_url = \"https://sanskritdocuments.org/iast/\"\n",
    "\n",
    "# Create output folder\n",
    "output_folder = r\"docs/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Improved headers to prevent 406 error\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0124bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each variable\n",
    "for variable in variables:\n",
    "    url = base_url + variable + \"/\"\n",
    "    print(f\"Processing URL: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        html_links = []\n",
    "\n",
    "        # Extract all PDF links\n",
    "        for ul in soup.find_all(\"ul\"):\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                pdf_tag = li.find(\"a\", href=lambda href: href and href.endswith(\".html\"))\n",
    "                if pdf_tag:\n",
    "                    html_links.append(pdf_tag['href'])\n",
    "\n",
    "        # Handle relative URLs\n",
    "        html_links = [\"https://sanskritdocuments.org\" + link if not link.startswith(\"http\") else link for link in html_links]\n",
    "\n",
    "        # Download PDFs\n",
    "        # for pdf_url in html_links:\n",
    "        #     filename = os.path.join(output_folder, pdf_url.split(\"/\")[-1])\n",
    "        #     print(f\"Downloading: {pdf_url}\")\n",
    "\n",
    "        #     try:\n",
    "        #         pdf_response = requests.get(pdf_url, headers=headers)\n",
    "        #         pdf_response.raise_for_status()\n",
    "\n",
    "        #         with open(filename, \"wb\") as f:\n",
    "        #             f.write(pdf_response.content)\n",
    "        #         print(f\"Saved to: {filename}\")\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")\n",
    "\n",
    "print(\"html list prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8643d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003499e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "# ---\n",
    "# Import necessary libraries for making HTTP requests and parsing HTML.\n",
    "# Make sure you have these installed ('pip install requests beautifulsoup4')\n",
    "# ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import os # Added for potential file saving later\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the base URL and a list of page suffixes,\n",
    "# visits each page, parses the HTML, and extracts links ending in '.html'\n",
    "# based on the specific structure (<div class='index-content'> -> <ul> -> <li> -> <a>).\n",
    "# ---\n",
    "def extract_html_links(base_url, page_suffixes):\n",
    "    \"\"\"\n",
    "    Fetches pages based on suffixes, parses them, and extracts .html links\n",
    "    from the specified structure.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the website (e.g., \"https://sanskritdocuments.org/\").\n",
    "        page_suffixes (list): A list of strings, where each string is a suffix\n",
    "                               to append to the base_url to get a page URL.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the full page URLs visited and\n",
    "              values are lists of the extracted .html hrefs found on that page.\n",
    "              Returns an empty dictionary if errors occur or no links are found.\n",
    "    \"\"\"\n",
    "    all_extracted_links = {}\n",
    "\n",
    "    # Ensure the base URL ends with a slash for proper joining\n",
    "    if not base_url.endswith('/'):\n",
    "        base_url += '/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "    }\n",
    "    print(f\"Starting extraction from base URL: {base_url}\")\n",
    "\n",
    "    for suffix in page_suffixes:\n",
    "        # Construct the full URL for the index page\n",
    "        page_url = urljoin(base_url, suffix.lstrip('/')) # Use urljoin for robust URL construction\n",
    "        print(f\"\\nProcessing page: {page_url}\")\n",
    "\n",
    "        page_html_links = [] # List to store .html links for the current page\n",
    "\n",
    "        try:\n",
    "            # Send an HTTP GET request to the page URL\n",
    "            # Use a session object for potential performance improvements (connection reuse)\n",
    "            with requests.Session() as session:\n",
    "                response = session.get(page_url, headers=headers, timeout=15) # Increased timeout slightly\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find the specific div with class=\"index-content\"\n",
    "            index_content_div = soup.find('div', class_='index-content')\n",
    "\n",
    "            if not index_content_div:\n",
    "                print(f\"  - Warning: Could not find <div class='index-content'> on {page_url}\")\n",
    "                all_extracted_links[page_url] = {\"status\": \"warning\", \"message\": \"index-content div not found\", \"links\": []}\n",
    "                continue # Move to the next suffix\n",
    "\n",
    "            # Find the <ul> tag within that div\n",
    "            ul_tag = index_content_div.find('ul')\n",
    "            # print(ul_tag)\n",
    "            if not ul_tag:\n",
    "                print(f\"  - Warning: Could not find <ul> within <div class='index-content'> on {page_url}\")\n",
    "                all_extracted_links[page_url] = {\"status\": \"warning\", \"message\": \"ul tag not found in index-content\", \"links\": []}\n",
    "                continue # Move to the next suffix\n",
    "\n",
    "            # Find all <li> tags within that <ul>\n",
    "            # Search recursively first, as the exact structure might vary slightly\n",
    "            li_tags = ul_tag.find_all('li')\n",
    "\n",
    "            if not li_tags:\n",
    "                 print(f\"  - Warning: Could not find any <li> tags within <ul> on {page_url}\")\n",
    "                 all_extracted_links[page_url] = {\"status\": \"warning\", \"message\": \"li tags not found in ul\", \"links\": []}\n",
    "                 continue # Move to the next suffix\n",
    "\n",
    "            # Iterate through each <li> tag found\n",
    "            for li in li_tags:\n",
    "                # Find all <a> tags within the current <li> that have an href\n",
    "                a_tags = li.find_all('a', href=True)\n",
    "\n",
    "                # Iterate through each <a> tag found\n",
    "                for a in a_tags:\n",
    "                    href = a['href'] # Get the value of the href attribute\n",
    "                    # Check if the href ends with '.html'\n",
    "                    if href.endswith('.html'):\n",
    "                        # Optionally resolve relative URLs fully here if needed later\n",
    "                        # full_link_url = urljoin(page_url, href) # Example\n",
    "                        print(f\"  - Found link: {href}\")\n",
    "                        page_html_links.append(href) # Store the relative href as found\n",
    "\n",
    "            # Store the found links for this page URL\n",
    "            all_extracted_links[page_url] = {\"status\": \"success\", \"message\": f\"Found {len(page_html_links)} '.html' links.\", \"links\": page_html_links}\n",
    "            print(f\"  - Finished processing {page_url}. Found {len(page_html_links)} '.html' links.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  - Error fetching page {page_url}: {e}\")\n",
    "            all_extracted_links[page_url] = {\"status\": \"error\", \"message\": f\"RequestException: {e}\", \"links\": []}\n",
    "        except Exception as e:\n",
    "            print(f\"  - An unexpected error occurred while processing {page_url}: {e}\")\n",
    "            all_extracted_links[page_url] = {\"status\": \"error\", \"message\": f\"Unexpected error: {e}\", \"links\": []}\n",
    "\n",
    "    return all_extracted_links\n",
    "\n",
    "print(\"Function 'extract_html_links' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1573981",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_WEBSITE_URL = \"https://sanskritdocuments.org/\"\n",
    "\n",
    "# Example suffixes (replace with your actual list)\n",
    "PAGE_SUFFIXES = variables\n",
    "\n",
    "print(\"Configuration set:\")\n",
    "print(f\"Base URL: {BASE_WEBSITE_URL}\")\n",
    "print(f\"Page Suffixes to process: {len(PAGE_SUFFIXES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1702482",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting the extraction process...\")\n",
    "\n",
    "extracted_data = extract_html_links(BASE_WEBSITE_URL, PAGE_SUFFIXES)\n",
    "\n",
    "print(\"\\n--- Extraction Summary ---\")\n",
    "if extracted_data:\n",
    "    successful_extractions = 0\n",
    "    warnings = 0\n",
    "    errors = 0\n",
    "    total_links_found = 0\n",
    "\n",
    "    for page, result in extracted_data.items():\n",
    "        print(f\"\\nPage: {page}\")\n",
    "        print(f\"  Status: {result.get('status', 'unknown')}\")\n",
    "        print(f\"  Message: {result.get('message', 'N/A')}\")\n",
    "\n",
    "        if result.get('status') == 'success':\n",
    "            successful_extractions += 1\n",
    "            links = result.get('links', [])\n",
    "            total_links_found += len(links)\n",
    "            if links:\n",
    "                print(f\"  Links ({len(links)}):\")\n",
    "                for link in links:\n",
    "                    print(f\"    - {link}\")\n",
    "            else:\n",
    "                print(\"  - No '.html' links found matching the criteria.\")\n",
    "        elif result.get('status') == 'warning':\n",
    "            warnings += 1\n",
    "        elif result.get('status') == 'error':\n",
    "            errors += 1\n",
    "\n",
    "    print(\"\\n--- Overall Stats ---\")\n",
    "    print(f\"Total pages processed: {len(extracted_data)}\")\n",
    "    print(f\"Successful extractions: {successful_extractions}\")\n",
    "    print(f\"Pages with warnings: {warnings}\")\n",
    "    print(f\"Pages with errors: {errors}\")\n",
    "    print(f\"Total '.html' links found across all successful pages: {total_links_found}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data was extracted. Check logs or script configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Extraction Summary ---\")\n",
    "if extracted_data:\n",
    "    successful_extractions = 0\n",
    "    warnings = 0\n",
    "    errors = 0\n",
    "    total_links_found = 0\n",
    "\n",
    "    for page, result in extracted_data.items():\n",
    "        print(f\"\\nPage: {page}\")\n",
    "        print(f\"  Status: {result.get('status', 'unknown')}\")\n",
    "        print(f\"  Message: {result.get('message', 'N/A')}\")\n",
    "\n",
    "        if result.get('status') == 'success':\n",
    "            successful_extractions += 1\n",
    "            links = result.get('links', [])\n",
    "            total_links_found += len(links)\n",
    "            if links:\n",
    "                print(f\"  Links ({len(links)}):\")\n",
    "                for link in links:\n",
    "                    print(f\"    - {link}\")\n",
    "            else:\n",
    "                print(\"  - No '.html' links found matching the criteria.\")\n",
    "        elif result.get('status') == 'warning':\n",
    "            warnings += 1\n",
    "        elif result.get('status') == 'error':\n",
    "            errors += 1\n",
    "\n",
    "    print(\"\\n--- Overall Stats ---\")\n",
    "    print(f\"Total pages processed: {len(extracted_data)}\")\n",
    "    print(f\"Successful extractions: {successful_extractions}\")\n",
    "    print(f\"Pages with warnings: {warnings}\")\n",
    "    print(f\"Pages with errors: {errors}\")\n",
    "    print(f\"Total '.html' links found across all successful pages: {total_links_found}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data was extracted. Check logs or script configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76678c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_links = []\n",
    "base_url = \"https://sanskritdocuments.org/\"\n",
    "\n",
    "for page_url, data in extracted_data.items():\n",
    "    if data.get('status') == 'success':\n",
    "        for link in data.get('links', []):\n",
    "            if not link.startswith('https'):\n",
    "                full_link = urljoin(base_url, link)\n",
    "                internal_links.append(full_link)\n",
    "\n",
    "print(\"\\n--- Internal Links (prefixed with base URL) ---\")\n",
    "if internal_links:\n",
    "    for link in internal_links:\n",
    "        print(link)\n",
    "    print(f\"\\nTotal internal links found: {len(internal_links)}\")\n",
    "else:\n",
    "    print(\"No internal links found based on the extracted data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sanskrit_text(html_url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content from a URL and extracts text within elements\n",
    "    that have the attribute lang='sa'.\n",
    "\n",
    "    Args:\n",
    "        html_url (str): The URL of the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is a block of Sanskrit text found.\n",
    "              Returns an empty list if there's an error or no Sanskrit text is found.\n",
    "    \"\"\"\n",
    "    sanskrit_texts = []\n",
    "\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            response = session.get(html_url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all elements that have the lang attribute set to 'sa'\n",
    "            sanskrit_elements = soup.find_all(attrs={'lang': 'sa'})\n",
    "\n",
    "            for element in sanskrit_elements:\n",
    "                # Extract the text content of each Sanskrit element and trim whitespace\n",
    "                text = element.get_text(separator='\\n', strip=True)\n",
    "                if text:  # Only add if the text is not empty\n",
    "                    sanskrit_texts.append(text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {html_url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {html_url}: {e}\")\n",
    "\n",
    "    return sanskrit_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(internal_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ca4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of HTML links in a variable called 'html_links'\n",
    "# Replace this with your actual list of links\n",
    "html_links = internal_links\n",
    "\n",
    "all_sanskrit_data = {}\n",
    "\n",
    "for link in html_links:\n",
    "    print(f\"\\nProcessing: {link}\")\n",
    "    sanskrit_data = get_sanskrit_text(link)\n",
    "    if sanskrit_data:\n",
    "        print(\"Found Sanskrit text:\")\n",
    "        for text_block in sanskrit_data:\n",
    "            print(\"-\" * 20)\n",
    "            print(text_block)\n",
    "        all_sanskrit_data[link] = sanskrit_data\n",
    "    else:\n",
    "        print(\"No Sanskrit text found on this page (with lang='sa').\")\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "if all_sanskrit_data:\n",
    "    print(f\"Sanskrit text extracted from {len(all_sanskrit_data)} pages.\")\n",
    "else:\n",
    "    print(\"No Sanskrit text was extracted from any of the provided links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14362617",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sanskrit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c79313",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_sanskrit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sanskrit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sanskrit_shloka(text):\n",
    "    \"\"\"\n",
    "    Extracts Sanskrit shlokas from a text by identifying lines that\n",
    "    predominantly contain Devanagari script.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing Sanskrit shlokas and other content.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the extracted Sanskrit shlokas, with each shloka\n",
    "             separated by a newline.\n",
    "    \"\"\"\n",
    "    sanskrit_shlokas = []\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Check if the line contains a significant portion of Devanagari characters\n",
    "        devanagari_count = 0\n",
    "        total_count = 0\n",
    "        for char in line:\n",
    "            if '\\u0900' <= char <= '\\u097F':  # Devanagari Unicode range\n",
    "                devanagari_count += 1\n",
    "            if char.strip():  # Count only non-whitespace characters\n",
    "                total_count += 1\n",
    "\n",
    "        # Heuristic: If more than 60% of non-whitespace characters are Devanagari,\n",
    "        # consider it a Sanskrit shloka line. Adjust this threshold as needed.\n",
    "        if total_count > 0 and (devanagari_count / total_count) > 0.7:\n",
    "            sanskrit_shlokas.append(line)\n",
    "\n",
    "    return \"\\n\".join(sanskrit_shlokas)\n",
    "\n",
    "text_to_extract = all_sanskrit_data[\"https://sanskritdocuments.org/doc_devii/apItakuchAmbAstava.html\"][0]\n",
    "\n",
    "extracted_shlokas = extract_sanskrit_shloka(text_to_extract)\n",
    "print(extracted_shlokas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fdc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = all_sanskrit_data[\"https://sanskritdocuments.org/doc_devii/apItakuchAmbAstava.html\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('docs/sans_data.json', \"w\") as file: \n",
    "        json.dump(all_sanskrit_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f302e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs/sans_data.json', 'r') as file:\n",
    "        loaded_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d530cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "two = loaded_dict[\"https://sanskritdocuments.org/doc_devii/apItakuchAmbAstava.html\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4aa55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if one==two:\n",
    "    print('yay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ea66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def contains_english(text):\n",
    "    \"\"\"\n",
    "    Checks if a string contains any English alphabet characters (a-z, A-Z).\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the string contains English letters, False otherwise.\n",
    "    \"\"\"\n",
    "    return bool(re.search(r'[a-zA-Z]', text))\n",
    "\n",
    "# Load the all_sanskrit_data from the JSON file\n",
    "try:\n",
    "    with open('docs/sans_data.json', 'r') as file:\n",
    "        all_sanskrit_data = json.load(file)\n",
    "    display(Markdown(\"**Loaded `all_sanskrit_data` from 'docs/sans_data.json'**\"))\n",
    "except FileNotFoundError:\n",
    "    display(Markdown(\"<span style='color:red'>**Error:** 'docs/sans_data.json' not found. Please ensure the file exists.</span>\"))\n",
    "    all_sanskrit_data = {}\n",
    "except json.JSONDecodeError:\n",
    "    display(Markdown(\"<span style='color:red'>**Error:** Could not decode JSON from 'docs/sans_data.json'. The file might be corrupted.</span>\"))\n",
    "    all_sanskrit_data = {}\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"<span style='color:red'>**Error:** An unexpected error occurred while loading JSON: {e}</span>\"))\n",
    "    all_sanskrit_data = {}\n",
    "\n",
    "# Apply extract_sanskrit_shloka to all texts\n",
    "extracted_shloka_data = {}\n",
    "if all_sanskrit_data:\n",
    "    display(Markdown(\"\\n**Applying `extract_sanskrit_shloka` to all extracted texts...**\"))\n",
    "    for url, texts in all_sanskrit_data.items():\n",
    "        extracted_shlokas_for_url = []\n",
    "        if isinstance(texts, list):\n",
    "            for text in texts:\n",
    "                extracted = extract_sanskrit_shloka(text)\n",
    "                if extracted:\n",
    "                    extracted_shlokas_for_url.append(extracted)\n",
    "        elif isinstance(texts, str):\n",
    "            extracted = extract_sanskrit_shloka(texts)\n",
    "            if extracted:\n",
    "                extracted_shlokas_for_url.append(extracted)\n",
    "        extracted_shloka_data[url] = extracted_shlokas_for_url\n",
    "    display(Markdown(\"**Extraction complete.**\"))\n",
    "else:\n",
    "    display(Markdown(\"<span style='color:orange'>**Warning:** `all_sanskrit_data` is empty. No texts to process.</span>\"))\n",
    "\n",
    "# Save the extracted shloka data to a new JSON file\n",
    "output_filename = 'docs/sanskrit_shlokas.json'\n",
    "try:\n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        json.dump(extracted_shloka_data, outfile, indent=4, ensure_ascii=False)\n",
    "    display(Markdown(f\"**Extracted Sanskrit shlokas saved to '{output_filename}'**\"))\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"<span style='color:red'>**Error:** Could not save extracted shlokas to '{output_filename}': {e}</span>\"))\n",
    "\n",
    "# Check for English in the extracted shlokas\n",
    "if extracted_shloka_data:\n",
    "    display(Markdown(\"\\n**Checking for English words in the extracted shlokas...**\"))\n",
    "    english_found = {}\n",
    "    for url, shlokas in extracted_shloka_data.items():\n",
    "        english_in_url = []\n",
    "        for shloka in shlokas:\n",
    "            if contains_english(shloka):\n",
    "                english_in_url.append(shloka.split('\\n')) # Check line by line\n",
    "        if english_in_url:\n",
    "            english_found[url] = english_in_url\n",
    "\n",
    "    if english_found:\n",
    "        display(Markdown(\"<span style='color:orange'>**Warning:** Potential English words found in the extracted shlokas:</span>\"))\n",
    "        for url, lines_with_english in english_found.items():\n",
    "            display(Markdown(f\"**URL:** `{url}`\"))\n",
    "            display(Markdown(\"```\"))\n",
    "            for lines in lines_with_english:\n",
    "                for line in lines:\n",
    "                    if contains_english(line):\n",
    "                        print(line)\n",
    "            display(Markdown(\"```\"))\n",
    "    else:\n",
    "        display(Markdown(\"**No English words found in the extracted shlokas (based on simple alphabet check).**\"))\n",
    "else:\n",
    "    display(Markdown(\"<span style='color:orange'>**Warning:** No extracted shlokas to check for English.</span>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72067cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def remove_non_devanagari(text):\n",
    "    \"\"\"Removes any character that is not Devanagari, newline, or common Sanskrit punctuation.\"\"\"\n",
    "    return \"\".join(char for char in text if '\\u0900' <= char <= '\\u097F' or char in ['\\n', ' ', 'ред', 'рее', ',', ';', ':', '-', '_', '(', ')', '[', ']'])\n",
    "\n",
    "def contains_english(text):\n",
    "    \"\"\"Checks if a string contains any English alphabet characters (a-z, A-Z).\"\"\"\n",
    "    return bool(re.search(r'[a-zA-Z]', text))\n",
    "\n",
    "# Load the extracted shloka data from the JSON file\n",
    "try:\n",
    "    with open('docs/sanskrit_shlokas.json', 'r') as file:\n",
    "        extracted_shloka_data = json.load(file)\n",
    "    display(Markdown(\"**Loaded `extracted_shloka_data` from 'docs/sanskrit_shlokas.json'**\"))\n",
    "except FileNotFoundError:\n",
    "    display(Markdown(\"<span style='color:red'>**Error:** 'docs/sanskrit_shlokas.json' not found. Please ensure the file exists.</span>\"))\n",
    "    extracted_shloka_data = {}\n",
    "except json.JSONDecodeError:\n",
    "    display(Markdown(\"<span style='color:red'>**Error:** Could not decode JSON from 'docs/sanskrit_shlokas.json'. The file might be corrupted.</span>\"))\n",
    "    extracted_shloka_data = {}\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"<span style='color:red'>**Error:** An unexpected error occurred while loading JSON: {e}</span>\"))\n",
    "    extracted_shloka_data = {}\n",
    "\n",
    "# Strictly remove non-Devanagari characters, keeping newlines\n",
    "sanskrit_only_data = {}\n",
    "if extracted_shloka_data:\n",
    "    display(Markdown(\"\\n**Strictly removing non-Devanagari characters (keeping newlines)...**\"))\n",
    "    for url, shlokas_list in extracted_shloka_data.items():\n",
    "        cleaned_shlokas = []\n",
    "        if isinstance(shlokas_list, list):\n",
    "            for shloka in shlokas_list:\n",
    "                cleaned_shlokas.append(remove_non_devanagari(shloka).strip())\n",
    "        elif isinstance(shlokas_list, str):\n",
    "            cleaned_shlokas.append(remove_non_devanagari(shlokas_list).strip())\n",
    "        sanskrit_only_data[url] = [s for s in cleaned_shlokas if s] # Remove empty strings\n",
    "    display(Markdown(\"**Non-Devanagari removal complete (keeping newlines).**\"))\n",
    "else:\n",
    "    display(Markdown(\"<span style='color:orange'>**Warning:** `extracted_shloka_data` is empty. No texts to process.</span>\"))\n",
    "\n",
    "# Save the strictly Sanskrit-only data to a new JSON file\n",
    "output_filename = 'docs/sanskrit_only_strict.json'\n",
    "try:\n",
    "    with open(output_filename, \"w\", encoding='utf-8') as outfile:\n",
    "        json.dump(sanskrit_only_data, outfile, indent=4, ensure_ascii=False)\n",
    "    display(Markdown(f\"**Strictly Sanskrit-only data saved to '{output_filename}'**\"))\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"<span style='color:red'>**Error:** Could not save strictly Sanskrit-only data to '{output_filename}': {e}</span>\"))\n",
    "\n",
    "# Verification (Optional): Check for any remaining English\n",
    "remaining_english = {}\n",
    "if sanskrit_only_data:\n",
    "    display(Markdown(\"\\n**Verifying for any remaining English characters...**\"))\n",
    "    for url, shlokas in sanskrit_only_data.items():\n",
    "        english_in_url = []\n",
    "        for shloka in shlokas:\n",
    "            if contains_english(shloka):\n",
    "                english_in_url.append(shloka)\n",
    "        if english_in_url:\n",
    "            remaining_english[url] = english_in_url\n",
    "\n",
    "    if remaining_english:\n",
    "        display(Markdown(\"<span style='color:orange'>**Warning:** English characters still found after strict removal:</span>\"))\n",
    "        for url, lines_with_english in remaining_english.items():\n",
    "            display(Markdown(f\"**URL:** `{url}`\"))\n",
    "            display(Markdown(\"```\"))\n",
    "            for line in lines_with_english[:5]:\n",
    "                print(line)\n",
    "            display(Markdown(\"```\"))\n",
    "    else:\n",
    "        display(Markdown(\"**No English characters found after strict removal.**\"))\n",
    "else:\n",
    "    display(Markdown(\"<span style='color:orange'>**Warning:** No Sanskrit-only data to verify.</span>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76995fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "# Load the strictly Sanskrit-only data from the JSON file\n",
    "try:\n",
    "    with open('docs/sanskrit_only_strict.json', 'r', encoding='utf-8') as file:\n",
    "        sanskrit_only_data = json.load(file)\n",
    "    display(Markdown(\"**Loaded strictly Sanskrit-only data.**\"))\n",
    "except FileNotFoundError:\n",
    "    display(Markdown(\"<span style='color:red'>**Error:** 'docs/sanskrit_only_strict.json' not found.</span>\"))\n",
    "    sanskrit_only_data = {}\n",
    "except json.JSONDecodeError:\n",
    "    display(Markdown(\"<span style='color:red'>**Error:** Could not decode JSON from 'docs/sanskrit_only_strict.json'.</span>\"))\n",
    "    sanskrit_only_data = {}\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"<span style='color:red'>**Error:** An unexpected error occurred: {e}</span>\"))\n",
    "    sanskrit_only_data = {}\n",
    "\n",
    "# Create a directory to save the text files if it doesn't exist\n",
    "output_dir = 'docs/text_file'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "display(Markdown(f\"**Saving Sanskrit texts to the '{output_dir}' directory.**\"))\n",
    "\n",
    "# Generate text files for each list of Sanskrit texts\n",
    "if sanskrit_only_data:\n",
    "    file_counter = 1\n",
    "    for url, shlokas_list in sanskrit_only_data.items():\n",
    "        if isinstance(shlokas_list, list):\n",
    "            for i, shloka in enumerate(shlokas_list):\n",
    "                filename = os.path.join(output_dir, f\"sanskrit_text_{file_counter:04d}.txt\")\n",
    "                try:\n",
    "                    with open(filename, 'w', encoding='utf-8') as outfile:\n",
    "                        outfile.write(shloka)\n",
    "                    display(Markdown(f\"Saved: `{filename}` (from URL: `{url}`, entry {i+1})\"))\n",
    "                    file_counter += 1\n",
    "                except Exception as e:\n",
    "                    display(Markdown(f\"<span style='color:red'>**Error saving** `{filename}`: {e}</span>\"))\n",
    "        elif isinstance(shlokas_list, str):\n",
    "            filename = os.path.join(output_dir, f\"sanskrit_text_{file_counter:04d}.txt\")\n",
    "            try:\n",
    "                with open(filename, 'w', encoding='utf-8') as outfile:\n",
    "                    outfile.write(shlokas_list)\n",
    "                display(Markdown(f\"Saved: `{filename}` (from URL: `{url}`)\"))\n",
    "                file_counter += 1\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"<span style='color:red'>**Error saving** `{filename}`: {e}</span>\"))\n",
    "    display(Markdown(\"**Sanskrit text file generation complete.**\"))\n",
    "else:\n",
    "    display(Markdown(\"<span style='color:orange'>**Warning:** `sanskrit_only_data` is empty. No texts to save.</span>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446868f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
